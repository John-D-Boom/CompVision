{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tt9a3p7726qD7uz-kKN1wuLZjrwVEIHy",
      "authorship_tag": "ABX9TyOGRzGE7QY/gubT8qd49TPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/John-D-Boom/CompVision/blob/load_all_data_before/testing_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook checks that I can load the dataset, load torch, syncing with GitHub, etc.\n"
      ],
      "metadata": {
        "id": "rIS_WB6uNXtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf6E8_aejHc0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from skimage import io\n",
        "from skimage.color import gray2rgb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('/content/drive/MyDrive/MLMI/CompVision/')"
      ],
      "metadata": {
        "id": "aMXojVcG2i5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/MLMI/CompVision/heatmap.csv')"
      ],
      "metadata": {
        "id": "a0VihMZ1O2Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # importing the \"tarfile\" module\n",
        "# import tarfile\n",
        "\n",
        "# # open file\n",
        "# file = tarfile.open('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200-copy.tar.gz')\n",
        "\n",
        "# # extracting file\n",
        "# file.extractall('/content/drive/MyDrive/MLMI/CompVision')\n",
        "\n",
        "# file.close()\n"
      ],
      "metadata": {
        "id": "mz82hSYNnktT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkFolderSetup(root_dir, expected_num):\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "      # For each subfolder in the root directory\n",
        "        for d in dirs:\n",
        "          # Get the full path to the subfolder\n",
        "            subfolder_path = os.path.join(root, d)\n",
        "          # Count the number of entries in the subfolder\n",
        "            num_entries = len(os.listdir(subfolder_path))\n",
        "          # Check if the number of entries in the subfolder is equal to n\n",
        "            try:\n",
        "                assert num_entries == expected_num \n",
        "            except:\n",
        "                print(f'{root_dir} has error in directory {d}')\n",
        "    print(f'Checked all folders in {root_dir}')\n",
        "\n"
      ],
      "metadata": {
        "id": "WpPOjP212ia3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_dir = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test'\n",
        "# val_dir =  '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/val'\n",
        "# train_dir =  '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/train'\n",
        "# checkFolderSetup(train_dir, 500)\n",
        "# checkFolderSetup(test_dir, 25)\n",
        "# checkFolderSetup(val_dir, 25)\n"
      ],
      "metadata": {
        "id": "Zfli1oT14Hnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shutil.copytree('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200', '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200-backup')"
      ],
      "metadata": {
        "id": "22DhRPSLe54i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import io\n",
        "path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/train/n03544143/n03544143_369.JPEG'\n",
        "img = io.imread(path)\n",
        "plt.figure(0)\n",
        "plt.imshow(img)\n",
        "plt.figure(1)\n",
        "plt.imshow(gray2rgb(img))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "ZM38AGMvkcNf",
        "outputId": "004d3539-3c9a-43b7-cbfa-ee8b8184cb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f32c027ad60>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRfNusz9rYVX",
        "outputId": "0175c386-5fb5-4bcb-fd92-867721dd1bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.13.0+cu116\n",
            "Torchvision Version:  0.14.0+cu116\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class siameseDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file, transform=None, device = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images_frame = pd.read_csv(csv_file, names = [\"Image1 Path\", \"Label1\", \"Image2 Path\", \"Label2\"])[:100]\n",
        "        all_data = torch.zeros((3,len(self.images_frame)))\n",
        "        all_tensor = torch.zeros(0) #temp value\n",
        "        \n",
        "        train_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/train.csv'\n",
        "        # if(csv_file == train_csv):\n",
        "            # print('is train')\n",
        "            # self.sim_df = self.images_frame[0:80000:2]\n",
        "            # self.dif_df = self.images_frame[80000:len(self.images_frame):10]\n",
        "            # self.images_frame = pd.concat([self.sim_df, self.dif_df])\n",
        "\n",
        "        #Unpack data into tensor to load into csv\n",
        "        for idx in range(len(self.images_frame)):\n",
        "            \n",
        "            defaultPathStart = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/'\n",
        "            img_path1 = self.images_frame.iloc[idx, 0].split('/tiny-imagenet-200/')[1] #Unpacks path and fixes to be compatible with google drive\n",
        "            img_path1 = defaultPathStart + img_path1\n",
        "            label1 = self.images_frame.iloc[idx, 1]\n",
        "            \n",
        "            img_path2 = self.images_frame.iloc[idx, 2].split('/tiny-imagenet-200/')[1] #Unpacks path and fixes to be compatible with google drive\n",
        "            img_path2 = defaultPathStart + img_path2\n",
        "            label2 = self.images_frame.iloc[idx, 3]\n",
        "\n",
        "            image1 = io.imread(img_path1)\n",
        "            image2 = io.imread(img_path2)\n",
        "\n",
        "            #Check shapes and enforce RGB\n",
        "\n",
        "            if (len(image1.shape) != 3):\n",
        "                # print(\"bad image1\")\n",
        "                image1 = gray2rgb(image1)\n",
        "            if (len(image2.shape) != 3):\n",
        "                # print(\"bad image2\")\n",
        "                image2 = gray2rgb(image2)\n",
        "\n",
        "\n",
        "            flag = ((label1==label2)*2 -1)\n",
        "\n",
        "            #tensors must all have the same shape, so if I want to send everything to gpu in one tensor I have to pad. \n",
        "            #Sucky but I don't expect memory issues\n",
        "            flag_tensor = flag_arr = torch.full((64,64,3), flag)\n",
        "\n",
        "            #make a 3, 64, 64 sample_tensor with \n",
        "            # 0: image1\n",
        "            # 1: image2\n",
        "            # 2: a 64x64 array of the flag (RIP)\n",
        "            sample_tensor = torch.stack((torch.tensor(image1),torch.tensor(image2),flag_tensor))\n",
        "            # print(\"sample shape\", sample_tensor.shape)\n",
        "            # print(\"all shape\", all_tensor.shape)\n",
        "            #add this tensor to the list\n",
        "            if idx == 0:\n",
        "                all_tensor = sample_tensor\n",
        "                continue\n",
        "            elif idx == 1:\n",
        "                all_tensor = all_tensor.unsqueeze(dim=0)\n",
        "            else:\n",
        "                all_tensor = torch.cat((all_tensor, sample_tensor.unsqueeze(dim=0)))\n",
        "        \n",
        "        self.images_frame = all_tensor\n",
        "        self.images_frame.to(device)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # print('getting item')\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        sample = self.images_frame[idx]\n",
        "        image1 = sample[0]\n",
        "        image2 = sample[1]\n",
        "        flag = sample[2][0,0]\n",
        "            \n",
        "        # print('imread successful')\n",
        "        print(image1.shape)\n",
        "        print(image2.shape)\n",
        "        print(flag.shape)\n",
        "        if self.transform:\n",
        "            try:\n",
        "                image1 = self.transform(image1)\n",
        "                image2 = self.transform(image2)\n",
        "                # print('transforming')\n",
        "            except:\n",
        "                print('ERROR in transforming')\n",
        "                 \n",
        "\n",
        "        sample = {'image1': image1, 'image2': image2, 'flag': flag}\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "V2rcofuArBce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"resnet\"\n",
        "\n",
        "# Number of classes in the dataset ***Dimensions of last layer***\n",
        "num_classes = 256\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 64\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 10\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model, \n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = False"
      ],
      "metadata": {
        "id": "a57CEw3akZwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_similar_acc_history = []\n",
        "    val_different_acc_history = []\n",
        "    \n",
        "    best_similar_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_different_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    best_similar_loss = 1e8\n",
        "    best_different_loss = 1e8\n",
        "\n",
        "    training_grayscale_errors = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        epoch_start = time.time()\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val_different', 'val_similar']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            # print(\"just before dataloader\")\n",
        "            for batch, obj in enumerate(dataloaders[phase]):\n",
        "                print(batch, \"/\", len(dataloaders[phase]))\n",
        "                try:\n",
        "                    # print(\"trying unpack obj\")\n",
        "                    image1 = obj[\"image1\"]\n",
        "                    image2 = obj[\"image2\"]\n",
        "                    flag = obj[\"flag\"]\n",
        "                except:\n",
        "                    training_grayscale_errors += 1\n",
        "                    continue\n",
        "\n",
        "                image1 = image1.to(device)\n",
        "                image2 = image2.to(device)\n",
        "                flag = flag.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "\n",
        "                    #***ignoring inception model rn\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        # print('running model1')\n",
        "                        output1 = model(image1)\n",
        "                        # print('running model2')\n",
        "                        output2 = model(image2)\n",
        "\n",
        "                        #***CRITERION HARDCODED AS COSINEEMBEDDINGLOSS***\n",
        "                        criterion = nn.CosineEmbeddingLoss(margin=0.0, reduction= 'mean')\n",
        "\n",
        "                        loss = criterion(output1, output2, flag)\n",
        "                        # print(\"loss:\",loss)\n",
        "                    # _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        \n",
        "                        optimizer.step()\n",
        "                        # print('updated weights')\n",
        "                    \n",
        "                # statistics\n",
        "                running_loss += loss.item() * output1.size(0) #multiplies scalar loss by the number in this batch\n",
        "                # running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = float(running_corrects) / len(dataloaders[phase].dataset)\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "            if phase == 'train':\n",
        "                print('Epoch complete in {:.0f}m {:.0f}s'.format(epoch_time // 60, epoch_time % 60))\n",
        "\n",
        "\n",
        "            # deep copy the model if in a validation mode\n",
        "            if phase == 'val_similar' and epoch_loss < best_similar_loss:\n",
        "                best_similar_acc = epoch_acc\n",
        "                best_similar_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(best_similar_model_wts, '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/first_resnet18/best_sim_weights')\n",
        "            if phase == 'val_similar':\n",
        "                val_similar_acc_history.append(epoch_acc)\n",
        "            if phase == 'val_different' and epoch_acc < best_different_loss:\n",
        "                best_different_acc = epoch_acc\n",
        "                best_different_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(best_different_model_wts, '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/first_resnet18/best_dif_weights')\n",
        "\n",
        "            if phase == 'val_different':\n",
        "                val_different_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_similar_acc))\n",
        "    print('Number of Greyscale Errors:', training_grayscale_errors)\n",
        "\n",
        "    # load best model weights\n",
        "  \n",
        "    \n",
        "\n",
        "    return best_similar_model_wts, best_different_model_wts, val_similar_acc_history, val_different_acc_history"
      ],
      "metadata": {
        "id": "ylVCM88DrgSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "uwxzpOuPkjSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3 \n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3hfLfZlbpgr",
        "outputId": "f3d622ed-8874-41ad-c86f-208450db3aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=256, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "\n",
        "\n",
        "train_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/train.csv'\n",
        "val_different_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/val_different.csv'\n",
        "val_similar_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/val_similar.csv'\n",
        "\n",
        "siamese_datasets = {}\n",
        "siamese_datasets[\"train\"] = siameseDataset(csv_file= train_csv, transform=data_transforms['train'])\n",
        "siamese_datasets[\"val_different\"] = siameseDataset(csv_file= val_different_csv, transform=data_transforms['val'])\n",
        "siamese_datasets[\"val_similar\"] = siameseDataset(csv_file= val_similar_csv, transform=data_transforms['val'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT3vpdvpk526",
        "outputId": "f4febef2-f4ab-4e83-e110-c0fd484a364f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Initializing Datasets and Dataloaders...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(siamese_datasets[x], batch_size=batch_size, shuffle=True, num_workers = 2, pin_memory = True) for x in ['train', 'val_different', 'val_similar']}\n"
      ],
      "metadata": {
        "id": "dJHqicGyxSXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are \n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ7G25enk9HQ",
        "outputId": "c5a77dac-c16b-4b8f-fb17-108e4e0c7e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params to learn:\n",
            "\t conv1.weight\n",
            "\t bn1.weight\n",
            "\t bn1.bias\n",
            "\t layer1.0.conv1.weight\n",
            "\t layer1.0.bn1.weight\n",
            "\t layer1.0.bn1.bias\n",
            "\t layer1.0.conv2.weight\n",
            "\t layer1.0.bn2.weight\n",
            "\t layer1.0.bn2.bias\n",
            "\t layer1.1.conv1.weight\n",
            "\t layer1.1.bn1.weight\n",
            "\t layer1.1.bn1.bias\n",
            "\t layer1.1.conv2.weight\n",
            "\t layer1.1.bn2.weight\n",
            "\t layer1.1.bn2.bias\n",
            "\t layer2.0.conv1.weight\n",
            "\t layer2.0.bn1.weight\n",
            "\t layer2.0.bn1.bias\n",
            "\t layer2.0.conv2.weight\n",
            "\t layer2.0.bn2.weight\n",
            "\t layer2.0.bn2.bias\n",
            "\t layer2.0.downsample.0.weight\n",
            "\t layer2.0.downsample.1.weight\n",
            "\t layer2.0.downsample.1.bias\n",
            "\t layer2.1.conv1.weight\n",
            "\t layer2.1.bn1.weight\n",
            "\t layer2.1.bn1.bias\n",
            "\t layer2.1.conv2.weight\n",
            "\t layer2.1.bn2.weight\n",
            "\t layer2.1.bn2.bias\n",
            "\t layer3.0.conv1.weight\n",
            "\t layer3.0.bn1.weight\n",
            "\t layer3.0.bn1.bias\n",
            "\t layer3.0.conv2.weight\n",
            "\t layer3.0.bn2.weight\n",
            "\t layer3.0.bn2.bias\n",
            "\t layer3.0.downsample.0.weight\n",
            "\t layer3.0.downsample.1.weight\n",
            "\t layer3.0.downsample.1.bias\n",
            "\t layer3.1.conv1.weight\n",
            "\t layer3.1.bn1.weight\n",
            "\t layer3.1.bn1.bias\n",
            "\t layer3.1.conv2.weight\n",
            "\t layer3.1.bn2.weight\n",
            "\t layer3.1.bn2.bias\n",
            "\t layer4.0.conv1.weight\n",
            "\t layer4.0.bn1.weight\n",
            "\t layer4.0.bn1.bias\n",
            "\t layer4.0.conv2.weight\n",
            "\t layer4.0.bn2.weight\n",
            "\t layer4.0.bn2.bias\n",
            "\t layer4.0.downsample.0.weight\n",
            "\t layer4.0.downsample.1.weight\n",
            "\t layer4.0.downsample.1.bias\n",
            "\t layer4.1.conv1.weight\n",
            "\t layer4.1.bn1.weight\n",
            "\t layer4.1.bn1.bias\n",
            "\t layer4.1.conv2.weight\n",
            "\t layer4.1.bn2.weight\n",
            "\t layer4.1.bn2.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN MODEL\n",
        "# Setup the loss fxn\n",
        "#***NOT CURRENTLY USING THIS CRITERION. HARD CODED IN TRAIN AS COSINEEMBEDDINGLOSS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "best_model_sim_weights, best_model_dif_weights, sim_hist, dif_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
        "\n",
        "#save weights\n",
        "torch.save(best_model_sim_weights, '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/first_resnet18/best_sim_weights')\n",
        "torch.save(best_model_dif_weights, '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/first_resnet18/best_dif_weights')\n",
        "np.savetxt('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/first_resnet18/sim_hist.txt', np.array(sim_hist))\n",
        "np.savetxt('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/first_resnet18/dif_hist.txt', np.array(dif_hist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HoYTGJCoaV4",
        "outputId": "3c321754-c367-4203-ff85-474601f8a7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])torch.Size([3])\n",
            "ERROR in transforming\n",
            "\n",
            "ERROR in transformingtorch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "\n",
            "torch.Size([3])\n",
            "ERROR in transformingERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "\n",
            "ERROR in transformingtorch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "\n",
            "torch.Size([64, 64, 3])ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])ERROR in transformingERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])torch.Size([3])\n",
            "ERROR in transforming\n",
            "ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])torch.Size([3])\n",
            "\n",
            "ERROR in transformingERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])\n",
            "ERROR in transformingtorch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "torch.Size([64, 64, 3])\n",
            "ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "\n",
            "torch.Size([3])\n",
            "ERROR in transformingERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "torch.Size([3])ERROR in transforming\n",
            "ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])torch.Size([3])\n",
            "ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])ERROR in transforming\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])torch.Size([3])\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])\n",
            "torch.Size([64, 64, 3])\n",
            "ERROR in transformingtorch.Size([3])\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "ERROR in transforming\n",
            "\n",
            "torch.Size([3])torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "\n",
            "torch.Size([3])\n",
            "ERROR in transformingERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "ERROR in transforming\n",
            "\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "ERROR in transformingtorch.Size([3])\n",
            "\n",
            "ERROR in transformingtorch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "ERROR in transforming\n",
            "\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "\n",
            "ERROR in transformingtorch.Size([3])\n",
            "\n",
            "torch.Size([64, 64, 3])ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "\n",
            "ERROR in transformingtorch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "ERROR in transformingtorch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])torch.Size([3])ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "\n",
            "torch.Size([3])\n",
            "ERROR in transformingERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])\n",
            "ERROR in transformingtorch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "\n",
            "torch.Size([64, 64, 3])ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "\n",
            "torch.Size([3])\n",
            "ERROR in transformingERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "\n",
            "torch.Size([3])\n",
            "ERROR in transformingERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "\n",
            "torch.Size([64, 64, 3])ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])torch.Size([64, 64, 3])\n",
            "\n",
            "ERROR in transformingtorch.Size([3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "ERROR in transformingtorch.Size([3])\n",
            "\n",
            "torch.Size([64, 64, 3])ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])\n",
            "torch.Size([64, 64, 3])\n",
            "ERROR in transformingtorch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])ERROR in transforming\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([3])\n",
            "torch.Size([64, 64, 3])ERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])\n",
            "torch.Size([3])\n",
            "ERROR in transformingERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([64, 64, 3])torch.Size([64, 64, 3])\n",
            "\n",
            "torch.Size([3])torch.Size([3])\n",
            "\n",
            "ERROR in transformingERROR in transforming\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "ERROR in transformingtorch.Size([64, 64, 3])torch.Size([3])\n",
            "\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])ERROR in transforming\n",
            "torch.Size([3])\n",
            "\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([3])\n",
            "\n",
            "ERROR in transforming\n",
            "torch.Size([64, 64, 3])\n",
            "torch.Size([64, 64, 3])torch.Size([3])\n",
            "ERROR in transforming\n",
            "0 / 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-016d37558434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbest_model_sim_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model_dif_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdif_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"inception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#save weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-d8c466cb23e5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception)\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                         \u001b[0;31m# print('running model1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                         \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                         \u001b[0;31m# print('running model2')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                         \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 64, 64, 3] to have 3 channels, but got 64 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the non-pretrained version of the model used for this run\n",
        "# scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
        "# scratch_model = scratch_model.to(device)\n",
        "# scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
        "# scratch_criterion = nn.CrossEntropyLoss()\n",
        "# _,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
        "\n",
        "# Plot the training curves of validation accuracy vs. number \n",
        "#  of training epochs for the transfer learning method and\n",
        "#  the model trained from scratch\n",
        "ohist = []\n",
        "# shist = []\n",
        "\n",
        "ohist = [h.cpu().numpy() for h in hist]\n",
        "# shist = [h.cpu().numpy() for h in scratch_hist]\n",
        "\n",
        "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
        "# plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
        "plt.ylim((0,1.))\n",
        "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nOb2Y7wsmHuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import io\n",
        "path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/train/n03544143/n03544143_435.JPEG'\n",
        "img = io.imread(path)\n",
        "plt.imshow(img)\n",
        "print(img.shape)"
      ],
      "metadata": {
        "id": "uou2e0G44EEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_hist = np.loadtxt('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/first_resnet18/sim_hist.txt')\n",
        "dif_hist = np.loadtxt('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/first_resnet18/dif_hist.txt')\n",
        "plt.figure(0)\n",
        "plt.plot(sim_hist, 'r.')\n",
        "plt.plot(dif_hist, 'g--')"
      ],
      "metadata": {
        "id": "6ZqxzyZM4FWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTArwy_I4BDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}