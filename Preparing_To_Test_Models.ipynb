{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RSdg0b65PH4z4Fq3h9WHFMLVijCKmbJz",
      "authorship_tag": "ABX9TyPoPO707iVPmyuCgJlNRFPT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/John-D-Boom/CompVision/blob/main/Preparing_To_Test_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will setup all of the code that will load the models with the finetuned weights and test them on a variety of test sets. First, the optimal similarity threshold needs to be determined to find the right AUC by running the model on the validation set and returning a tensor of all of the similarities and another tensor of the ground truth labels. This is basically already in the other one\n",
        "\n",
        "Then, iterate through different cutoffs and select the one that maximizes the geometric mean, np.sqrt(TPR * (1-FPR))\n",
        "\n",
        "Finally, given this threshold, evaluate the test sets. Save vector of loss for each pair to the correct folder, return the accuracy and average loss"
      ],
      "metadata": {
        "id": "kEonctfXjeLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://people.eecs.berkeley.edu/~hendrycks/imagenet-o.tar\n",
        "# !tar xf imagenet-o.tar \n",
        "\n",
        "# !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        " \n",
        "# !unzip -qq 'tiny-imagenet-200.zip'\n",
        "# !pip install torchmetrics\n",
        "# !pip install timm"
      ],
      "metadata": {
        "id": "_mv4o84IC0DL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fz15jkCjXoX",
        "outputId": "6927406b-d106-4772-ba53-3e2ed0defb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.13.0+cu116\n",
            "Torchvision Version:  0.14.0+cu116\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "from sklearn import metrics\n",
        "import timm\n",
        "from skimage.color import gray2rgb\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "# from torchmetrics.classification import BinaryAUROC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class siameseDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file, transform=None, device = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images_frame = pd.read_csv(csv_file, names = [\"Image1 Path\", \"Label1\", \"Image2 Path\", \"Label2\"])\n",
        "        train_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/train.csv'\n",
        "        if(csv_file == train_csv):\n",
        "            print('is train')\n",
        "            self.sim_df = self.images_frame[0:80000:2]\n",
        "            print('sim dim:', len(self.sim_df))\n",
        "            self.dif_df = self.images_frame[80000:len(self.images_frame):3]\n",
        "            print('dif dim:', len(self.dif_df))\n",
        "            self.images_frame = pd.concat([self.sim_df, self.dif_df])\n",
        "        \n",
        "\n",
        "        \n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # print('getting item')\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        img_path1 = \"\"\n",
        "        label1 = \"\"\n",
        "        img_path2 = \"\"\n",
        "        label2 = \"\"\n",
        "        \n",
        "        train_path = '/content/tiny-imagenet-200/train'\n",
        "        val_or_test_path = '/content/tiny-imagenet-200/val/images'   \n",
        "        \n",
        "        clean_path = self.images_frame.iloc[idx, 0]\n",
        "\n",
        "        if (len(clean_path.split('/')) < 3): #Comes from imagenet-o\n",
        "            # print('in imageneto')\n",
        "            imagenetO_root = '/content/imagenet-o'\n",
        "\n",
        "            img_path1 = os.path.join(imagenetO_root, self.images_frame.iloc[idx,0])\n",
        "            label1 = self.images_frame.iloc[idx, 1]\n",
        "\n",
        "            img_path2 = os.path.join(imagenetO_root, self.images_frame.iloc[idx,2])\n",
        "            label2 = self.images_frame.iloc[idx, 3]\n",
        "        else:\n",
        "        \n",
        "            img_path1 = self.images_frame.iloc[idx, 0].split('/tiny-imagenet-200/')[1] #Unpacks path and fixes to be compatible with google drive \n",
        "            label1 = self.images_frame.iloc[idx, 1]\n",
        "            \n",
        "            img_path2 = self.images_frame.iloc[idx, 2].split('/tiny-imagenet-200/')[1] #Unpacks path and fixes to be compatible with google drive\n",
        "            label2 = self.images_frame.iloc[idx, 3]\n",
        "\n",
        "            spec_img1 = img_path1.split('/')[-1]\n",
        "            spec_img2 = img_path2.split('/')[-1]\n",
        "            \n",
        "            if 'train' in img_path1:\n",
        "                class_img1 = spec_img1.split(\"_\")[0]\n",
        "                class_img2 = spec_img2.split(\"_\")[0]\n",
        "                img_path1 = os.path.join(train_path, class_img1, \"images\", spec_img1)\n",
        "                img_path2 = os.path.join(train_path, class_img2, \"images\", spec_img2)\n",
        "\n",
        "            elif 'val' in img_path1 or 'test' in img_path1:\n",
        "                # print('in val')\n",
        "                img_path1 = os.path.join(val_or_test_path, spec_img1)\n",
        "                img_path2 = os.path.join(val_or_test_path, spec_img2)\n",
        "            else:\n",
        "                raise ValueError('Bad Image Path. \"Test\", \"Train\" \"Val\" not found.')\n",
        "\n",
        "        flag = ((label1 == label2) * 2) -1\n",
        "\n",
        "        # if flag == -1:\n",
        "        #     print(\"negative flag loaded\")\n",
        "        # print(img_path1, img_path2)\n",
        "\n",
        "        image1 = io.imread(img_path1)\n",
        "        image2 = io.imread(img_path2)\n",
        "\n",
        "        #Check shapes and enforce RGB\n",
        "\n",
        "        if (len(image1.shape) != 3):\n",
        "            # print(\"bad image1:\", image1.shape)\n",
        "            image1 = gray2rgb(image1)\n",
        "        if (len(image2.shape) != 3):\n",
        "            # print(\"bad image2\", image2.shape)\n",
        "            image2 = gray2rgb(image2)\n",
        "            \n",
        "        # print(image1.shape)\n",
        "        # print(image2.shape)\n",
        "        # print('imread successful')\n",
        "        if self.transform:\n",
        "            try:\n",
        "                \n",
        "                image1 = self.transform(image1)\n",
        "                # print(\"image1 successful\")\n",
        "                image2 = self.transform(image2)\n",
        "                # print('transforming')\n",
        "            except:\n",
        "                # print(image1.shape)\n",
        "                # print(image2.shape)\n",
        "                print('ERROR in transforming:', idx, img_path1)\n",
        "        \n",
        "        # print(\"final:\" , image1.shape, image2.shape)\n",
        "        sample = {'image1': image1, 'image2': image2, 'flag': flag}\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "_Psmvf-houvv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions to initialize models"
      ],
      "metadata": {
        "id": "svunpUlGpGTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "bZB6V2OqqbJZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "    if model_name == \"resnet18\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        resnet18_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/resnet18/best_auc_weights'\n",
        "        resnet18_state_dict = torch.load(resnet18_state_dict_path)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        model_ft.load_state_dict(copy.deepcopy(resnet18_state_dict))\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        " \n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"resnet50\":\n",
        "        \"\"\" Resnet50\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "        resnet50_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/resnet50/best_auc_weights'\n",
        "        resnet50_state_dict = torch.load(resnet50_state_dict_path)\n",
        "        model_ft.load_state_dict(copy.deepcopy(resnet50_state_dict))\n",
        "\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "\n",
        "        input_size = 224\n",
        "    \n",
        "    elif model_name == \"resnet101\":\n",
        "        \"\"\" Resnet101\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet101(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "\n",
        "        alexnet_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/alexnet/best_auc_weights'\n",
        "        alexnet_state_dict = torch.load(alexnet_state_dict_path)\n",
        "        model_ft.load_state_dict(copy.deepcopy(alexnet_state_dict))\n",
        "\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "\n",
        "        squeezenet_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/squeezenet/best_auc_weights'\n",
        "        squeezenet_state_dict = torch.load(squeezenet_state_dict_path)\n",
        "        model_ft.load_state_dict(copy.deepcopy(squeezenet_state_dict))\n",
        "\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "\n",
        "        input_size = 224\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3 \n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "    \n",
        "    elif model_name == \"vit\":\n",
        "        \"\"\"Vision Transformer\n",
        "        \"\"\"\n",
        "        model_ft = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    elif model_name == \"efficientnet\":\n",
        "        \"\"\"efficientnet_b4\n",
        "        \"\"\"\n",
        "        model_ft = timm.create_model('efficientnet_b0', pretrained=True, num_classes=num_classes)\n",
        "        efficient_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/efficientnet/best_auc_weights'\n",
        "        efficient_state_dict = torch.load(efficient_state_dict_path)\n",
        "\n",
        "        model_ft.load_state_dict(copy.deepcopy(efficient_state_dict))\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        print(model_name)\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "# model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "# print(model_ft)"
      ],
      "metadata": {
        "id": "otYg2BOLpEqO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions to find optimal threshold "
      ],
      "metadata": {
        "id": "8P45mNk6qeIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def geomMean(tpr, fpr):\n",
        "    # print(np.sqrt(tpr * (1-fpr)))\n",
        "    return np.sqrt(tpr * (1-fpr))\n",
        "\n",
        "def findOptimalThreshold(groundtruth, predictions):\n",
        "    #Takes in two arrays and finds optimal threshold\n",
        "    assert len(groundtruth) == len(predictions)\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(groundtruth, predictions)\n",
        "    # print(\"fpr:\", fpr)\n",
        "    # print(\"tpr:\", tpr)\n",
        "    # print(\"thresholds:\", thresholds)\n",
        "    best_score = 0\n",
        "    best_threshold = 0\n",
        "    for i in range(len(thresholds)):\n",
        "        threshold = thresholds[i]\n",
        "        score = geomMean(tpr[i], fpr[i])\n",
        "        if score > best_score:\n",
        "            best_threshold = threshold\n",
        "            best_score = score\n",
        "    return best_threshold"
      ],
      "metadata": {
        "id": "wKMXg_UXqccT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize GPU"
      ],
      "metadata": {
        "id": "7dfvQyIxc21S"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cch7T_Lbc0vc",
        "outputId": "5864b8d0-11fc-41b1-e58d-b20db8a74e0a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model_name, model, dataloader, criterion):\n",
        "    #Takes in a model and outputs the groundtruth labels alongside the models prediction \n",
        "    since = time.time()\n",
        "\n",
        "    validation_labels = torch.empty(0).to(device)\n",
        "    validation_pred_distance = torch.empty(0).to(device)\n",
        "\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "\n",
        "    # Iterate over data.\n",
        "    # print(\"just before dataloader\")\n",
        "    for batch, obj in enumerate(dataloader):\n",
        "        # print(batch, \"/\", len(dataloaders[phase]))\n",
        "        try:\n",
        "            # print(\"trying unpack obj\")\n",
        "            image1 = obj[\"image1\"]\n",
        "            image2 = obj[\"image2\"]\n",
        "            flag = obj[\"flag\"]\n",
        "        except:\n",
        "            training_grayscale_errors += 1\n",
        "            continue\n",
        "\n",
        "        image1 = image1.to(device)\n",
        "        image2 = image2.to(device)\n",
        "        flag = flag.to(device)\n",
        "\n",
        "\n",
        "        # track history off since model in eval mode\n",
        "        with torch.set_grad_enabled(False):\n",
        "\n",
        "            # print('running model1')\n",
        "            output1 = model(image1)\n",
        "            # print('running model2')\n",
        "            output2 = model(image2)\n",
        "\n",
        "            #Add predictions if in a validation mode\n",
        "            distance_func = nn.CosineSimilarity(dim = 1, eps = 1e-6)\n",
        "            predicted_similarity = distance_func(output1, output2)\n",
        "            # print(predicted_similarity.shape)\n",
        "            # print(flag.shape)\n",
        "            # assert predicted_similarity.shape == (batch_size,1) #ensure that it is the same size\n",
        "            labels = (flag + 1) / 2 #converts to 0 and 1\n",
        "            validation_labels = torch.cat((validation_labels,labels))\n",
        "            validation_pred_distance = torch.cat((validation_pred_distance, predicted_similarity))\n",
        "\n",
        "\n",
        "    return validation_labels, validation_pred_distance"
      ],
      "metadata": {
        "id": "dpbWLoQ4q36b"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok now that helper functions are setup, it's time to setup dataloaders for validation"
      ],
      "metadata": {
        "id": "G9paWKDQwFgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "val_different_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/val_different.csv'\n",
        "val_similar_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/val_similar.csv'\n",
        "val_different_dataset = siameseDataset(csv_file= val_different_csv, transform=transform)\n",
        "val_similar_dataset = siameseDataset(csv_file= val_similar_csv, transform=transform)\n",
        "\n",
        "val_different_dataloader = torch.utils.data.DataLoader(val_different_dataset, batch_size=64, shuffle=False, num_workers = 2, pin_memory = True)\n",
        "val_similar_dataloader = torch.utils.data.DataLoader(val_similar_dataset, batch_size=64, shuffle=False, num_workers = 2, pin_memory = True)"
      ],
      "metadata": {
        "id": "BPfYPFElwW-l"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to initialize model, get the optimal threshold on the validation set, and save those predictions."
      ],
      "metadata": {
        "id": "eO15IW4tzJac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_imagenetO_dataset = siameseDataset(csv_file= '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_imagenetO_similar.csv', transform=transform)\n",
        "# test_imagenetO_dataset[0]"
      ],
      "metadata": {
        "id": "u-T3YDCvGUAY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = []\n",
        "for model_name in model_names:\n",
        "    criterion = nn.CosineEmbeddingLoss()\n",
        "    model_ft, input_size = initialize_model(model_name, num_classes = 128, feature_extract = False, use_pretrained = True)\n",
        "    model_ft.to(device)\n",
        "    different_groundtruth, different_predictions = eval_model(model_name, model_ft, val_different_dataloader, criterion)\n",
        "    similar_groundtruth, similar_predictions = eval_model(model_name, model_ft, val_similar_dataloader, criterion)\n",
        "    combined_groundtruth = torch.cat((different_groundtruth, similar_groundtruth))\n",
        "    combined_predictions = torch.cat((different_predictions, similar_predictions))\n",
        "    combined_groundtruth = combined_groundtruth.cpu()\n",
        "    combined_predictions = combined_predictions.cpu()\n",
        "\n",
        "    model_dir = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/{}'.format(model_name)\n",
        "\n",
        "    opt_thresh = findOptimalThreshold(combined_groundtruth, combined_predictions)\n",
        "    np.savetxt(os.path.join(model_dir, 'opt_thresh.txt'), np.array([opt_thresh]))\n",
        "\n",
        "    validation_arr = np.array([combined_groundtruth.cpu().numpy(), combined_predictions.cpu().numpy()])\n",
        "    np.savetxt(os.path.join(model_dir, 'validation_predictions.txt'), validation_arr)\n",
        "    print(model_name, \"has an optimal threshold of\", opt_thresh)\n",
        "\n",
        "    #Cleanup to preserve ram.\n",
        "    model_ft.cpu()\n",
        "    del model_ft\n"
      ],
      "metadata": {
        "id": "e2DYr862wlJE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, with the optimal threshold found, the model can be tested on each one, after which it the predictions and labels will be saved"
      ],
      "metadata": {
        "id": "7dEKYsGrzGEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generateDataloaderDict():\n",
        "    # print('in dataloaderdict')\n",
        "    test_sets = [\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_seen_similar.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_unseen_similar.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_imagenetO_similar.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_seen_seen_different.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_unseen_seen_different.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_unseen_unseen_different.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_imagenetO_different.csv',\n",
        "                 ]\n",
        "    dataloaders_dict = {}\n",
        "    for csv_path in test_sets:\n",
        "        # print(csv_path)\n",
        "        dataset = siameseDataset(csv_path, transform = transform)\n",
        "        print(len(dataset))\n",
        "        \n",
        "        test_name = csv_path.split('/')[-1].split('.')[0]\n",
        "        # print(test_name)\n",
        "        # print(test_name, \"has length: \", len(dataset))\n",
        "        dataloaders_dict[test_name] = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers = 12, pin_memory = True)\n",
        "    return dataloaders_dict\n"
      ],
      "metadata": {
        "id": "SW3KY4vT7s2c"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testModel(dataloader, model_ft, model_name, test_name, opt_thresh):\n",
        "    # print(model_ft)\n",
        "    criterion = 'temp'\n",
        "    groundtruth, predictions = eval_model(model_name, model_ft, dataloader, criterion)\n",
        "    test_arr = np.array([groundtruth.cpu().numpy(), predictions.cpu().numpy()])\n",
        "    model_dir = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/{}'.format(model_name)\n",
        "    np.savetxt(os.path.join(model_dir, '{}_predictions.txt'.format(test_name)), test_arr)\n",
        "    predictions_mask = (predictions.cpu().numpy() > opt_thresh).astype(int)\n",
        "    accuracy = metrics.accuracy_score(groundtruth.cpu().numpy(), predictions_mask)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "def testModelWrapper(dataloader_dict, model_name, opt_thresh):\n",
        "    #Returns the accuracies in the order of the dataloader_dict\n",
        "    accuracies = []\n",
        "    model, input_size = initialize_model(model_name, num_classes = 128, feature_extract = False, use_pretrained = True)\n",
        "    model.to(device)\n",
        "    for test_name in dataloader_dict.keys():\n",
        "        print(test_name)\n",
        "        dataloader = dataloader_dict[test_name]\n",
        "        accuracy = testModel(dataloader, model, model_name, test_name, opt_thresh)\n",
        "        accuracies.append(accuracy)\n",
        "    return accuracies\n",
        "\n",
        "\n",
        "def testAllModels():\n",
        "\n",
        "    dataloader_dict = generateDataloaderDict()\n",
        "    print(\"dataloader_dict:\", dataloader_dict)\n",
        "    all_accuracies = []\n",
        "    model_names = [\"resnet18\", \"resnet50\", \"alexnet\", \"squeezenet\", \"efficientnet\"]\n",
        "    for model_name in model_names:\n",
        "        print(model_name)\n",
        "        model_dir = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/{}'.format(model_name)\n",
        "        opt_thresh = np.loadtxt(os.path.join(model_dir, 'opt_thresh.txt'))\n",
        "\n",
        "        accuracies = testModelWrapper(dataloader_dict, model_name, opt_thresh)\n",
        "        all_accuracies.append(accuracies)\n",
        "    accuracy_df = pd.DataFrame(all_accuracies, columns = dataloader_dict.keys())\n",
        "    accuracy_df.to_csv('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/model_accuracies.csv', index = False)"
      ],
      "metadata": {
        "id": "kUvbt8JRzFL6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testAllModels()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PphP4IEGM2N",
        "outputId": "2c9a53b6-cf8b-4737-dd62-285b907c0122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48000\n",
            "12000\n",
            "501\n",
            "25440\n",
            "19200\n",
            "7800\n",
            "4499\n",
            "dataloader_dict: {'test_seen_similar': <torch.utils.data.dataloader.DataLoader object at 0x7f3f4e14a370>, 'test_unseen_similar': <torch.utils.data.dataloader.DataLoader object at 0x7f3f4e14a280>, 'test_imagenetO_similar': <torch.utils.data.dataloader.DataLoader object at 0x7f3f4e14a940>, 'test_seen_seen_different': <torch.utils.data.dataloader.DataLoader object at 0x7f3f4e14aa90>, 'test_unseen_seen_different': <torch.utils.data.dataloader.DataLoader object at 0x7f3ec0790190>, 'test_unseen_unseen_different': <torch.utils.data.dataloader.DataLoader object at 0x7f3ec07901c0>, 'test_imagenetO_different': <torch.utils.data.dataloader.DataLoader object at 0x7f3ec0790700>}\n",
            "resnet18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_seen_similar\n",
            "test_unseen_similar\n",
            "test_imagenetO_similar\n",
            "test_seen_seen_different\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_dict = generateDataloaderDict()\n",
        "\n",
        "# sim_dict = data_dict['test_imagenetO_different']\n",
        "\n",
        "# for batch, obj in enumerate(sim_dict):\n",
        "#     print(batch)\n",
        "#     test = obj\n"
      ],
      "metadata": {
        "id": "DwDzrsw7r9-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_dict.keys()\n",
        "# df.iloc[309, 0]\n",
        "# df.iloc[310, 0]"
      ],
      "metadata": {
        "id": "uZShQXmtwnMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_imagenetO_different.csv', names =['image1', 'label1', 'image2', 'label2'])"
      ],
      "metadata": {
        "id": "p6TiV216wygO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLnuUyFn0-Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# print('yes' in df['image1'].unique())"
      ],
      "metadata": {
        "id": "n8L8rb8wx40N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.iloc[4435]"
      ],
      "metadata": {
        "id": "JicF84THx6xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_df = df.drop(4435)"
      ],
      "metadata": {
        "id": "IJ_wBGBwx9fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XhbUN4tiyAep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_df.iloc[311]"
      ],
      "metadata": {
        "id": "m72Bv4B0ySq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zqPDpNqC3voo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}