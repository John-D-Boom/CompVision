{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RSdg0b65PH4z4Fq3h9WHFMLVijCKmbJz",
      "authorship_tag": "ABX9TyNkHQxVWfZsvSQnYa7LEWr1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will setup all of the code that will load the models with the finetuned weights and test them on a variety of test sets. First, the optimal similarity threshold needs to be determined to find the right AUC by running the model on the validation set and returning a tensor of all of the similarities and another tensor of the ground truth labels. This is basically already in the other one\n",
        "\n",
        "Then, iterate through different cutoffs and select the one that maximizes the geometric mean, np.sqrt(TPR * (1-FPR))\n",
        "\n",
        "Finally, given this threshold, evaluate the test sets. Save vector of loss for each pair to the correct folder, return the accuracy and average loss"
      ],
      "metadata": {
        "id": "kEonctfXjeLJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fz15jkCjXoX",
        "outputId": "bfc9d4b8-e401-4556-b69e-c366d4d4b78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.13.0+cu116\n",
            "Torchvision Version:  0.14.0+cu116\n"
          ]
        }
      ],
      "source": [
        "# !pip install timm torchmetrics\n",
        "\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "from sklearn import metrics\n",
        "import timm\n",
        "from skimage.color import gray2rgb\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "from torchmetrics.classification import BinaryAUROC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Add support for the new dataset from imagenetO\n",
        "class siameseDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file, transform=None, device = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.images_frame = pd.read_csv(csv_file, names = [\"Image1 Path\", \"Label1\", \"Image2 Path\", \"Label2\"])\n",
        "        train_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/train.csv'\n",
        "        if(csv_file == train_csv):\n",
        "            print('is train')\n",
        "            self.sim_df = self.images_frame[0:80000:2]\n",
        "            print('sim dim:', len(self.sim_df))\n",
        "            self.dif_df = self.images_frame[80000:len(self.images_frame):3]\n",
        "            print('dif dim:', len(self.dif_df))\n",
        "            self.images_frame = pd.concat([self.sim_df, self.dif_df])\n",
        "        \n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # print('getting item')\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        train_path = '/content/tiny-imagenet-200/train'\n",
        "        val_or_test_path = '/content/tiny-imagenet-200/val/images'   \n",
        "        \n",
        "        \n",
        "        img_path1 = self.images_frame.iloc[idx, 0].split('/tiny-imagenet-200/')[1] #Unpacks path and fixes to be compatible with google drive \n",
        "        label1 = self.images_frame.iloc[idx, 1]\n",
        "        \n",
        "        img_path2 = self.images_frame.iloc[idx, 2].split('/tiny-imagenet-200/')[1] #Unpacks path and fixes to be compatible with google drive\n",
        "        label2 = self.images_frame.iloc[idx, 3]\n",
        "\n",
        "        spec_img1 = img_path1.split('/')[-1]\n",
        "        spec_img2 = img_path2.split('/')[-1]\n",
        "        \n",
        "        if 'train' in img_path1:\n",
        "            class_img1 = spec_img1.split(\"_\")[0]\n",
        "            class_img2 = spec_img2.split(\"_\")[0]\n",
        "            img_path1 = os.path.join(train_path, class_img1, \"images\", spec_img1)\n",
        "            img_path2 = os.path.join(train_path, class_img2, \"images\", spec_img2)\n",
        "        elif 'val' in img_path1 or 'test' in img_path1:\n",
        "            \n",
        "            img_path1 = os.path.join(val_or_test_path, spec_img1)\n",
        "            img_path2 = os.path.join(val_or_test_path, spec_img2)\n",
        "        else:\n",
        "            raise ValueError('Bad Image Path. \"Test\", \"Train\" \"Val\" not found.')\n",
        "\n",
        "        flag = ((label1 == label2) * 2) -1\n",
        "\n",
        "        # if flag == -1:\n",
        "        #     print(\"negative flag loaded\")\n",
        "        # print(img_path1, img_path2)\n",
        "\n",
        "        image1 = io.imread(img_path1)\n",
        "        image2 = io.imread(img_path2)\n",
        "\n",
        "        #Check shapes and enforce RGB\n",
        "\n",
        "        if (len(image1.shape) != 3):\n",
        "            # print(\"bad image1\")\n",
        "            image1 = gray2rgb(image1)\n",
        "        if (len(image2.shape) != 3):\n",
        "            # print(\"bad image2\")\n",
        "            image2 = gray2rgb(image2)\n",
        "            \n",
        "        # print('imread successful')\n",
        "        if self.transform:\n",
        "            try:\n",
        "                image1 = self.transform(image1)\n",
        "                image2 = self.transform(image2)\n",
        "                # print('transforming')\n",
        "            except:\n",
        "                print('ERROR in transforming')\n",
        "        \n",
        "        sample = {'image1': image1, 'image2': image2, 'flag': flag}\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "_Psmvf-houvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions to initialize models"
      ],
      "metadata": {
        "id": "svunpUlGpGTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "bZB6V2OqqbJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "    if model_name == \"resnet18\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        resnet18_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/resnet18/best_auc_weights'\n",
        "        resnet18_state_dict = torch.load(resnet18_state_dict_path)\n",
        "\n",
        "        model_ft.load_state_dict(copy.deepcopy(resnet18_state_dict))\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"resnet50\":\n",
        "        \"\"\" Resnet50\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "        resnet50_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/resnet50/best_auc_weights'\n",
        "        resnet50_state_dict = torch.load(resnet50_state_dict_path)\n",
        "\n",
        "        model_ft.load_state_dict(copy.deepcopy(resnet50_state_dict))\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "    \n",
        "    elif model_name == \"resnet101\":\n",
        "        \"\"\" Resnet101\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet101(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        alexnet_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/alexnet/best_auc_weights'\n",
        "        alexnet_state_dict = torch.load(alexnet_state_dict_path)\n",
        "\n",
        "        model_ft.load_state_dict(copy.deepcopy(alexnet_state_dict))\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        squeezenet_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/squeezenet/best_auc_weights'\n",
        "        squeezenet_state_dict = torch.load(squeezenet_state_dict_path)\n",
        "\n",
        "        model_ft.load_state_dict(copy.deepcopy(squeezenet_state_dict))\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3 \n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "    \n",
        "    elif model_name == \"vit\":\n",
        "        \"\"\"Vision Transformer\n",
        "        \"\"\"\n",
        "        model_ft = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    elif model_name == \"efficientnet\":\n",
        "        \"\"\"efficientnet_b4\n",
        "        \"\"\"\n",
        "        model_ft = timm.create_model('efficientnet_b0', pretrained=True, num_classes=num_classes)\n",
        "        efficient_state_dict_path = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/efficientnet/best_auc_weights'\n",
        "        efficient_state_dict = torch.load(efficient_state_dict_path)\n",
        "\n",
        "        model_ft.load_state_dict(copy.deepcopy(efficient_state_dict))\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        print(model_name)\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "# model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "# print(model_ft)"
      ],
      "metadata": {
        "id": "otYg2BOLpEqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions to find optimal threshold "
      ],
      "metadata": {
        "id": "8P45mNk6qeIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from traitlets.config.application import T\n",
        "def geomMean(tpr, fpr):\n",
        "    return np.sqrt(tpr * (1-fpr))\n",
        "\n",
        "def findOptimalThreshold(groundtruth, predictions):\n",
        "    #Takes in two arrays and finds optimal threshold\n",
        "    assert len(groundtruth) == len(predictions)\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(groundtruth, predictions)\n",
        "\n",
        "    best_score = 0\n",
        "    best_threshold = 0\n",
        "    for i in range(len(thresholds)):\n",
        "        threshold = thresholds[i]\n",
        "        score = geomMean(tpr, fpr)\n",
        "        if score > best_score:\n",
        "            best_threshold = threshold\n",
        "            best_score = score\n",
        "    return best_threshold"
      ],
      "metadata": {
        "id": "wKMXg_UXqccT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model_name, model, dataloader, criterion):\n",
        "    #Takes in a model and outputs the groundtruth labels alongside the models prediction \n",
        "    since = time.time()\n",
        "\n",
        "    validation_labels = torch.empty(0).to(device)\n",
        "    validation_pred_distance = torch.empty(0).to(device)\n",
        "\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "\n",
        "    # Iterate over data.\n",
        "    # print(\"just before dataloader\")\n",
        "    for batch, obj in enumerate(dataloader):\n",
        "        # print(batch, \"/\", len(dataloaders[phase]))\n",
        "        try:\n",
        "            # print(\"trying unpack obj\")\n",
        "            image1 = obj[\"image1\"]\n",
        "            image2 = obj[\"image2\"]\n",
        "            flag = obj[\"flag\"]\n",
        "        except:\n",
        "            training_grayscale_errors += 1\n",
        "            continue\n",
        "\n",
        "        image1 = image1.to(device)\n",
        "        image2 = image2.to(device)\n",
        "        flag = flag.to(device)\n",
        "\n",
        "\n",
        "        # track history off since model in eval mode\n",
        "        with torch.set_grad_enabled(False):\n",
        "\n",
        "            # print('running model1')\n",
        "            output1 = model(image1)\n",
        "            # print('running model2')\n",
        "            output2 = model(image2)\n",
        "\n",
        "            #Add predictions if in a validation mode\n",
        "            distance_func = nn.CosineSimilarity(dim = 1, eps = 1e-6)\n",
        "            predicted_similarity = distance_func(output1, output2)\n",
        "            # print(predicted_similarity.shape)\n",
        "            # print(flag.shape)\n",
        "            # assert predicted_similarity.shape == (batch_size,1) #ensure that it is the same size\n",
        "            labels = (flag + 1) / 2 #converts to 0 and 1\n",
        "            validation_labels = torch.cat((validation_labels,labels))\n",
        "            validation_pred_distance = torch.cat((validation_pred_distance, predicted_similarity))\n",
        "\n",
        "\n",
        "    return validation_labels, validation_pred_distance"
      ],
      "metadata": {
        "id": "dpbWLoQ4q36b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok now that helper functions are setup, it's time to setup dataloaders for validation"
      ],
      "metadata": {
        "id": "G9paWKDQwFgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "val_different_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/val_different.csv'\n",
        "val_similar_csv = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/val_similar.csv'\n",
        "val_different_dataset = siameseDataset(csv_file= val_different_csv, transform=transform)\n",
        "val_similar_dataset = siameseDataset(csv_file= val_similar_csv, transform=transform)\n",
        "\n",
        "val_different_dataloader = torch.utils.data.DataLoader(val_different_dataset, batch_size=64, shuffle=False, num_workers = 2, pin_memory = True)\n",
        "val_similar_dataloader = torch.utils.data.DataLoader(val_different_dataset, batch_size=64, shuffle=False, num_workers = 2, pin_memory = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "BPfYPFElwW-l",
        "outputId": "2d907779-f4cd-4998-a177-2a1738ea0bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-80009692fdf9>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    ]\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to initialize model, get the optimal threshold on the validation set, and save those predictions."
      ],
      "metadata": {
        "id": "eO15IW4tzJac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"resnet18\"\n",
        "criterion = nn.CosineEmbeddingLoss()\n",
        "model_ft, input_size = initialize_model(model_name, num_classes = 128, feature_extract = False, use_pretrained = True)\n",
        "different_groundtruth, different_predictions = eval_model(model_name, model_ft, val_different_dataloader, criterion)\n",
        "similar_groundtruth, similar_predictions = eval_model(model_name, model_ft, val_similar_dataloader, criterion)\n",
        "\n",
        "combined_groundtruth = different_groundtruth + similar_groundtruth\n",
        "combined_predictions = different_predictions + similar_predictions\n",
        "\n",
        "opt_thresh = findOptimalThreshold(combined_groundtruth, combined_predictions)\n",
        "\n",
        "validation_arr = np.array([combined_groundtruth, combined_predictions])\n",
        "model_dir = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/{}'.format(model_name)\n",
        "np.savetxt(os.path.join(model_dir, 'validation_predictions.txt'), validation_arr)\n",
        "print(model_name, \"has an optimal threshold of\", opt_thresh)\n",
        "#TODO: Add saving optimum threshold"
      ],
      "metadata": {
        "id": "e2DYr862wlJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, with the optimal threshold found, the model can be tested on each one, after which it the predictions and labels will be saved"
      ],
      "metadata": {
        "id": "7dEKYsGrzGEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generateDataloaderDict():\n",
        "    test_sets = ['/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_seen_similar.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_unseen_similar.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_imagenetO_similar.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_seen_seen_different.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_unseen_seen_different.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_unseen_unseen_different.csv',\n",
        "                 '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/test_imagenetO_different.csv',\n",
        "                 ]\n",
        "    dataloaders_dict = {}\n",
        "    for csv_path in test_sets:\n",
        "\n",
        "        dataset = siameseDataset(csv_path, transform = transform)\n",
        "        test_name = csv_path.split('/')[-1].split('.')[0]\n",
        "        dataloaders_dict[test_name] = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers = 2, pin_memory = True)\n",
        "\n"
      ],
      "metadata": {
        "id": "SW3KY4vT7s2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testModel(dataloader, model_ft, model_name, test_name, opt_thresh):\n",
        "\n",
        "    groundtruth, predictions = eval_model(model_name, model_ft, dataloader, criterion)\n",
        "    test_arr = np.array([groundtruth, predictions])\n",
        "    model_dir = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/{}'.format(model_name)\n",
        "    np.savetxt(os.path.join(model_dir, '{}_predictions.txt'.format(test_name)), test_arr)\n",
        "    predictions_mask = (predictions > opt_thresh).astype(int)\n",
        "    accuracy = metrics.accuracy_score(groundtruth, predictions)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "def testModelWrapper(dataloader_dict, model_name, opt_thresh):\n",
        "    #Returns the accuracies in the order of the dataloader_dict\n",
        "    accuracies = []\n",
        "    model = initialize_model(model_name, num_classes = 128, feature_extract = False, use_pretrained = True)\n",
        "    for test_name in dataloader_dict:\n",
        "        dataloader = dataloader_dict[test_name]\n",
        "        accuracy = testModel(dataloader, model, model_name, test_name, opt_thresh)\n",
        "        accuracies.append(accuracy)\n",
        "    return accuracies\n",
        "\n",
        "\n",
        "def testAllModels():\n",
        "\n",
        "    dataloader_dict = generateDataloaderDict()\n",
        "    all_accuracies = []\n",
        "    model_names = [\"resnet18\", \"resnet50\", \"alexnet\", \"squeezenet\", \"efficientnet\"]\n",
        "    for model_name in model_names:\n",
        "        model_dir = '/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/{}'.format(model_name)\n",
        "        opt_thresh = np.loadtxt(os.path.join(model_dir, 'opt_thresh.txt'))\n",
        "\n",
        "        accuracies = testModelWrapper(dataloader_dict, model_name, opt_thresh)\n",
        "        all_accuracies.append(accuracies)\n",
        "    accuracy_df = pd.DataFrame(all_accuracies, columns = dataloader_dict.keys())\n",
        "    accuracy_df.to_csv('/content/drive/MyDrive/MLMI/CompVision/tiny-imagenet-200/models/model_accuracies.csv', index = False)\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "kUvbt8JRzFL6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}